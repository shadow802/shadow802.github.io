<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"shadow802.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":true,"nav":{"gitalk":{"order":-1}},"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="前言    最近在进行K8s运维时，遇到一些问题，特此记录，以备忘。 问题记录负载均衡选择    我们在原生集群中，集成了Nginx Ingress Controller，并使用了公有云上的CLB进行转发。从CLB到K8s集群中的Nginx Controller实例，目前主流上如下三种连接方式：">
<meta property="og:type" content="article">
<meta property="og:title" content="K8s集群运维杂记">
<meta property="og:url" content="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/index.html">
<meta property="og:site_name" content="人生如梦 对酒当歌">
<meta property="og:description" content="前言    最近在进行K8s运维时，遇到一些问题，特此记录，以备忘。 问题记录负载均衡选择    我们在原生集群中，集成了Nginx Ingress Controller，并使用了公有云上的CLB进行转发。从CLB到K8s集群中的Nginx Controller实例，目前主流上如下三种连接方式：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/curl-httpbin.png">
<meta property="og:image" content="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/drain-node.png">
<meta property="og:image" content="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/curl-httpbin-error.png">
<meta property="og:image" content="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/drain-httpbin-log.png">
<meta property="og:image" content="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/pdb-drain.png">
<meta property="article:published_time" content="2022-06-18T16:00:00.000Z">
<meta property="article:modified_time" content="2022-06-18T16:00:00.000Z">
<meta property="article:author" content="Shadow">
<meta property="article:tag" content="CLB">
<meta property="article:tag" content="Prometheus">
<meta property="article:tag" content="Drain">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/curl-httpbin.png">

<link rel="canonical" href="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>K8s集群运维杂记 | 人生如梦 对酒当歌</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">人生如梦 对酒当歌</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://shadow802.github.io/2022/06/19/K8s%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4%E6%9D%82%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shadow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="人生如梦 对酒当歌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          K8s集群运维杂记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2022-06-19T00:00:00+08:00">2022-06-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%BF%90%E7%BB%B4%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index"><span itemprop="name">运维实践</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>    最近在进行K8s运维时，遇到一些问题，特此记录，以备忘。</p>
<h1 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h1><h2 id="负载均衡选择"><a href="#负载均衡选择" class="headerlink" title="负载均衡选择"></a>负载均衡选择</h2><p>    我们在原生集群中，集成了Nginx Ingress Controller，并使用了公有云上的CLB进行转发。从CLB到K8s集群中的Nginx Controller实例，目前主流上如下三种连接方式：</p>
<h3 id="HostNetwork"><a href="#HostNetwork" class="headerlink" title="HostNetwork"></a>HostNetwork</h3><p>    让Nginx Controller直接使用节点的网络。</p>
<p>    这种方式的好处是效率高，CLB直接转发给节点，进协议栈，然后交给Nginx实例。</p>
<p>    缺点是需要固定集群中的一些节点最为CLB的后端，一旦需要扩缩容，则需要手动维护CLB的后端服务器列表。而且如果无法在单个节点上部署多套Nginx Controller，会有端口冲突的问题（不建议在命令行修改Nginx Controller默认的端口号，不利于维护）。</p>
<h3 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h3><p>    主流厂商的LoadBalancer Service都是通过NodePort的方式实现的。优点是厂商一般都提供了LoadBalancer Service自动创建（绑定已创建）CLB的方式，CLB的后端无需手工维护，可以自动配置。</p>
<p>    缺点是，首先通过NodePort这种方式会导致网络通信链路变长，CLB转发给NodePort后需要在协议栈中再次进行转发，或是转发给同节点的Pod网络命名空间，或是跨节点转发给Nginx Controller实例。如果需要保留客户端IP需要设置externalTrafficPolicy=local，这样虽然保留住了客户端IP。但是如果在没有Nginx Controller的节点上访问CLB，会访问不通。究其原因，是因为LoadBalancer Service的ExternalName会写上CLB的IP，然后IPVS会在本机设置到CLB这个IP的转发地址，而Service的externalTrafficPolicy=local允许转发给本机的Nginx Controller。这些问题导致该方案无法使用。</p>
<h3 id="ENI"><a href="#ENI" class="headerlink" title="ENI"></a>ENI</h3><p>    这种方式对Pod的网络模型有要求，Pod的CIDR必须使用VPC中的子网。节点上会为Pod单独绑定弹性网卡，设置IP。</p>
<p>    Pod相当于与集群中的节点是对等的关系，在一个扁平网络里。这种方式SLB可以想转发给VPC中节点一样，直接将数据转发给Pod。</p>
<p>    这种方式Pod的IP从VPC的子网中直接分配，可能会有IP不足的问题。不同公有云厂商也有一些其他限制（例如必须使用公有云提供的操作系统内核）。具体情况需要参照公有云厂商的文档，这里不再详述。</p>
<h2 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h2><p>    最近搭建了一个海外的K8s集群，集群中部署了Prometheus，公司内网这边需要读取集群中的监控数据，海外集群和公司内网目前通过IPSec进行连接，延迟大约在200+ms。我们的proxy prometheus连接了许多prometheus，应用查询时需要遍历所有的prometheus，因此不能将高延迟的promethues放到proxy上，否则会拖慢所有相关查询的速度。</p>
<h3 id="Remote-Storage"><a href="#Remote-Storage" class="headerlink" title="Remote Storage"></a>Remote Storage</h3><p>    为了解决Prometheus历史数据持久化的问题，Prometheus定义了两个标准的接口remote write/remote read，用户可以基于这两个接口将数据保存到任意第三方的存储服务中，这种方式被称为Remote Storage。</p>
<h4 id="Remote-Write"><a href="#Remote-Write" class="headerlink" title="Remote Write"></a>Remote Write</h4><p>    Prometheus内置了Remote Write的实现，只要在服务端的Prometheus启动命令行加入如下参数，即可开启。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--web.enable-remote-write-receiver    # 默认的写入端点是：/api/v1/write</span><br></pre></td></tr></table></figure>

<p>    然后在客户端的Prometheus的配置文件中，加入写入的端点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">remote_write:</span><br><span class="line">- url: http://xx.xxx.xx.xxx:9090/api/v1/write</span><br><span class="line">  remote_timeout: 30s</span><br></pre></td></tr></table></figure>

<p>    客户端的Prometheus会启动一个队列，从wal中读取样本数据，然后写入内存中分片所属的队列，然后请求远程写入端点，发送数据。</p>
<p>  </p>
<blockquote>
<p>不影响客户端Prometheus本地数据的保存，会在远程端点额外保存一份。</p>
</blockquote>
<h4 id="Remote-Read"><a href="#Remote-Read" class="headerlink" title="Remote Read"></a>Remote Read</h4><p>    支持从第三方存储中读取Promethues数据（通过一个adapter服务将第三方存储的数据协议转换为Prometheus的标准数据协议，可自定义），当然也可以从另一个Prometheus中读取数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">remote_read:</span><br><span class="line">  - read_recent: true</span><br><span class="line">    remote_timeout: 30s</span><br><span class="line">    url: http://xx.xx.xx.xx:9090/api/v1/read</span><br></pre></td></tr></table></figure>

<p>    远程读取会从远端服务中获取原始样本数据，然后在本地进行计算。</p>
<h2 id="Node-Maintain"><a href="#Node-Maintain" class="headerlink" title="Node Maintain"></a>Node Maintain</h2><p>    最近接到需求，需要对生产集群部分节点进行下线处理，首先根据资源的情况，确定的部分节点，接下来要对节点上的业务负载进行处理，并退订该部分节点。</p>
<h3 id="Cordon"><a href="#Cordon" class="headerlink" title="Cordon"></a>Cordon</h3><p>    对节点执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl cordon kind-worker      # 使用kubectl uncordon kind-worker解除</span><br></pre></td></tr></table></figure>

<p>    查看Node，发现在taints中增加了一行，标明不再接受新调度的Pod。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Taints:             node.kubernetes.io/unschedulable:NoSchedule</span><br></pre></td></tr></table></figure>

<h3 id="Drain"><a href="#Drain" class="headerlink" title="Drain"></a>Drain</h3><p>    使用drain命令，会对当前节点上的Pod进行Evicted。无法驱逐使用了local-path的Pod和Daemonset（可使用–ignore-daemonsets忽略）。</p>
<p>    drain并不能保证应用的可用性，如果需要保证服务不中断，服务质量不受影响，需要结合PodDisruptionBudget。</p>
<h3 id="PodDisruptionBudget"><a href="#PodDisruptionBudget" class="headerlink" title="PodDisruptionBudget"></a>PodDisruptionBudget</h3><p>    PDB可以设置最大不可用副本数和最小可用副本数，一旦drain操作进行时，被驱逐的工作负载的副本，违反PDB中设置的规则，drain将会报错，无法进行下去，直到满足PDB。</p>
<h3 id="实践验证"><a href="#实践验证" class="headerlink" title="实践验证"></a>实践验证</h3><p>    受限于电脑资源，使用kind创建了一个两个节点的K8s集群，并去除调了Master节点不能调度Pod的taints。</p>
<p>    部署一个httpbin的deployment，设置副本数为1。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: httpbin</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: httpbin</span><br><span class="line">      version: v1</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: httpbin</span><br><span class="line">        version: v1</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: httpbin</span><br><span class="line">        image: xxx.xxx.xxx/release/httpbin</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        readinessProbe:</span><br><span class="line">          failureThreshold: 3</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 80</span><br><span class="line">          timeoutSeconds: 1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: http</span><br><span class="line">          protocol: TCP</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: httpbin</span><br><span class="line">  name: httpbin</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: http</span><br><span class="line">  selector:</span><br><span class="line">    app: httpbin</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure>

<p>    查看集群Pod的状态，并通过curl验证Pod访问正常。</p>
<p><img src="curl-httpbin.png" alt="curl-httpbin.png"></p>
<p>    执行kubectl drain kind-control-pane：</p>
<p><img src="drain-node.png" alt="drain-node.png"></p>
<p>    在drain执行的过程中，curl httpbin服务。发现中间出现了服务中断：</p>
<p><img src="curl-httpbin-error.png" alt="curl-httpbin-error.png"></p>
<p>    watch Pod的日志，发现新创建的Pod尚未Ready，就已经被停止。</p>
<p><img src="drain-httpbin-log.png" alt="drain-httpbin-log.png"></p>
<p>   这与我们的要求相违背，加上PDB再次进行实验：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: policy/v1</span><br><span class="line">kind: PodDisruptionBudget</span><br><span class="line">metadata:</span><br><span class="line">  name: httpbin</span><br><span class="line">spec:</span><br><span class="line">  minAvailable: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: httpbin</span><br></pre></td></tr></table></figure>

<p>    发现PDB阻止了drain对Pod进行Evicted。也就是说PDB可以防止drain进行误操作，破坏业务为应用副本制定的规则，从而保障了节点维护时，不会影响服务质量，发生服务中断这样的生产问题。</p>
<p><img src="pdb-drain.png" alt="pdb-drain.png"></p>
<p>    因此，为了能Evicted要维护节点上的Pod，可以修改Deployment的副本数，扩大到2，让其他节点上有httpbin的Ready实例，drain就能顺利执行。</p>
<h2 id="Zookeeper启动异常"><a href="#Zookeeper启动异常" class="headerlink" title="Zookeeper启动异常"></a>Zookeeper启动异常</h2><p>    K8s集群Pod间网络不通，strimzi-kafka-operator启动zookeeper时，日志报NullPointerExecption。从日志上一点也看不出来是网路不通的原因。</p>
<h2 id="Iptables"><a href="#Iptables" class="headerlink" title="Iptables"></a>Iptables</h2><p>    部署Drone的plugins/docker时，在镜像中通过如下命令连接宿主机sock时，会报Iptables命令无法设置，或者是不存在iptables，提示通过insmod安装。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dockerd --data-root /var/lib/docker --host=unix:///var/run/docker.sock</span><br></pre></td></tr></table></figure>

<p>    后经多番尝试可通过添加–iptables=false测试启动成功。查看宿主机和镜像中的iptables版本iptables v1.8.4 (nf_tables)和iptables v1.8.3 (legacy)。</p>
<p>    目前Linux的iptables有了nf-tables这种全新的实现，改进了原来的内核api，无法与原先的版本兼容。后通过在主机中加载ip_tables模块得以解决。目前宿主机中同时存在nf_tables和legacy的iptables，目前尚未发现问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe ip_tables</span><br></pre></td></tr></table></figure>

<h2 id="负载均衡内网回环"><a href="#负载均衡内网回环" class="headerlink" title="负载均衡内网回环"></a>负载均衡内网回环</h2><p>    使用公有云CLB+集群Nginx Ingress进行负载均衡时，会遇到内网负载均衡网络回环的问题，分一下几种情况：</p>
<p>    （1）公网CLB，CLB指向的后端服务节点以及其上面的Pod访问公网负载均衡地址，不会出现回环问题，因为从服务节点出去的流量会nat成公网的地址，再转发给CLB时，CLB转发给后端服务节点，后端服务节点无法识别到自己的公网地址，所以通信正常。</p>
<p>    （2）内网CLB，CLB指向的后端服务节点以及器上面的Pod访问内网负载均衡地址，源IP为该服务器的地址，如果CLB只有这一个后端，转发给服务节点后，服务节点会发现源IP和目的IP是一样的，因此会在服务器内部回环。如果存在多个后端，那么如果转发给其他服务节点，通信正常，如果转发给这个服务节点，会失败，七层CLB会重试，就会出现延迟比较高的情况。</p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/document/product/457/61737#.E4.B8.BA.E4.BB.80.E4.B9.88.E5.85.AC.E7.BD.91-clb-.E4.B8.8D.E5.AD.98.E5.9C.A8.E5.9B.9E.E7.8E.AF.E9.97.AE.E9.A2.98.EF.BC.9F">容器服务 CLB 回环问题 - 故障处理 - 文档中心 - 腾讯云</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CLB/" rel="tag"># CLB</a>
              <a href="/tags/Prometheus/" rel="tag"># Prometheus</a>
              <a href="/tags/Drain/" rel="tag"># Drain</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/05/tcpdump%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/" rel="prev" title="tcpdump抓包分析">
      <i class="fa fa-chevron-left"></i> tcpdump抓包分析
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/06/21/Linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" rel="next" title="Linux运维常用命令">
      Linux运维常用命令 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">问题记录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%80%89%E6%8B%A9"><span class="nav-number">2.1.</span> <span class="nav-text">负载均衡选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HostNetwork"><span class="nav-number">2.1.1.</span> <span class="nav-text">HostNetwork</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NodePort"><span class="nav-number">2.1.2.</span> <span class="nav-text">NodePort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ENI"><span class="nav-number">2.1.3.</span> <span class="nav-text">ENI</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prometheus"><span class="nav-number">2.2.</span> <span class="nav-text">Prometheus</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Remote-Storage"><span class="nav-number">2.2.1.</span> <span class="nav-text">Remote Storage</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Remote-Write"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">Remote Write</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Remote-Read"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">Remote Read</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Node-Maintain"><span class="nav-number">2.3.</span> <span class="nav-text">Node Maintain</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cordon"><span class="nav-number">2.3.1.</span> <span class="nav-text">Cordon</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Drain"><span class="nav-number">2.3.2.</span> <span class="nav-text">Drain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PodDisruptionBudget"><span class="nav-number">2.3.3.</span> <span class="nav-text">PodDisruptionBudget</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E8%B7%B5%E9%AA%8C%E8%AF%81"><span class="nav-number">2.3.4.</span> <span class="nav-text">实践验证</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Zookeeper%E5%90%AF%E5%8A%A8%E5%BC%82%E5%B8%B8"><span class="nav-number">2.4.</span> <span class="nav-text">Zookeeper启动异常</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Iptables"><span class="nav-number">2.5.</span> <span class="nav-text">Iptables</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%86%85%E7%BD%91%E5%9B%9E%E7%8E%AF"><span class="nav-number">2.6.</span> <span class="nav-text">负载均衡内网回环</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Shadow</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">74</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shadow</span>
</div>

<div class="powered-by">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <i class="fa fa-user-md"></i>
    <span id="busuanzi_container_site_uv">
        本站访客数:<span id="busuanzi_value_site_uv"></span>
    </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">
        本站访问量<span id="busuanzi_value_site_pv"></span>
    </span>
</div>
        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'Ov23li2RKrPIAmFLysIK',
      clientSecret: '82e3a65c3737ce4fa8e9beab672138f4617f3143',
      repo        : 'shadow802.github.io',
      owner       : 'shadow802',
      admin       : ['shadow802'],
      id          : '46f9bb20c74e24e974666dd7447d44ce',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
